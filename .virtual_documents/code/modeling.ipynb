


# Basics
import pandas as pd 
import numpy as np 
import seaborn as sns 
import matplotlib.pyplot as plt 

# for tokenizing and lemmantizing
from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import re

# for models 
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer





reddit_df = pd.read_csv('../data/preprocessed-data.csv')
reddit_df.head()


reddit_df.shape


reddit_df.isnull().sum()


# drop rows with no body 
reddit_df.dropna(subset = ['body'], inplace = True)
reddit_df.shape





# Create target column 
reddit_df['target'] = reddit_df['subreddit'].map({'personalfinance': 1, 'investing': 0})
reddit_df.head()


# Unique words for each subreddit
unique_investing_words = ['investor', 'resource', 'trading', 'list', 'relevant', 'useful', 'wiki', 'sampp', 'level', 'sector', 'trade', 'allocation', 'daily', 'vti', 'exposure', 'data', 'world', 'discussion', 'international', 'horizon']  
unique_personalfinance_words = ['vehicle', 'owe', 'afford', 'cc', 'deductible', 'auto', 'apr', 'collection', 'policy', 'charge', 'phone', 'claim', 'refinance', 'file', 'repair', 'food', 'asked', 'truck', 'mile', '15k']  

# Combine
unique_words = unique_investing_words + unique_personalfinance_words


# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, model_name): 
    conf_matrix = confusion_matrix(y_true, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix = conf_matrix)
    disp.plot(cmap = plt.cm.Blues)
    plt.title(f'Confusion Matrix for {model_name}')
    plt.savefig(f'../images/{model_name}-confusion-matrix.png', bbox_inches='tight')
    plt.tight_layout()
    plt.show





X = reddit_df['body_lemmatized']
y = reddit_df['target']


y.value_counts(normalize = True)


X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    stratify=y,
                                                    random_state=42)











# Baseline model: Logistic Regression, with CountVectorizer 
pipe = Pipeline([
    ('cvec', CountVectorizer()),
    ('scaler', StandardScaler(with_mean=False)), 
    ('lr', LogisticRegression(max_iter = 500))
])


# Pipe parameters
pipe_params = {
    'cvec__max_features': [500, 1000, 1500, 2000], 
    'cvec__stop_words': [None, 'english'], 
    'cvec__ngram_range': [(1,1),(1,2)],
    'cvec__lowercase': [True],
    'lr__C': [0.01, 0.1, 1, 10],  
    'lr__penalty': ['l1', 'l2'], 
    'lr__solver': ['liblinear']
}


# Instantiate GridSearchCV.
gs = GridSearchCV(pipe, 
                  pipe_params, 
                  cv = 5) 


# Fit the model
gs.fit(X_train, y_train)


# Print model scores
print(f'Best score: {gs.best_score_}')
print()
print(f'Best params: {gs.best_params_}')
print()
y_pred = gs.predict(X_test)
print(classification_report(y_test, y_pred))


plot_confusion_matrix(y_test, y_pred, 'Logistic Regression + CountVectorizer')








# Logistic Regression, with CountVectorizer using unique_words for vocabulary parameter
pipe_lr_cvec_vocab = Pipeline([
    ('cvec', CountVectorizer(vocabulary = unique_words)),
    ('scaler', StandardScaler(with_mean=False)), 
    ('lr', LogisticRegression(max_iter = 500))
])


pipe_params_lr_cvec_vocab = {
    'cvec__max_features': [1500], 
    'cvec__stop_words': ['english'], 
    'cvec__ngram_range': [(1,2)],
    'cvec__lowercase': [True],
    'lr__C': [0.01],  
    'lr__penalty': ['l2'], 
    'lr__solver': ['liblinear']
}


# Instantiate GridSearchCV.
gs_lr_cvec_vocab = GridSearchCV(pipe_lr_cvec_vocab, 
                                pipe_params_lr_cvec_vocab, 
                                cv = 5) 


gs_lr_cvec_vocab.fit(X_train, y_train)


print(f'Best Score: {gs_lr_cvec_vocab.best_score_}')
print()
print(f'Best params: {gs_lr_cvec_vocab.best_params_}')
print()
y_pred = gs_lr_cvec_vocab.predict(X_test)
print(classification_report(y_test, y_pred))


plot_confusion_matrix(y_test, y_pred, 'Logistic Regression + CountVectorizer (Custom Vocab)')








# Logistic Regression, with TVEC 
pipe_lr_tvec = Pipeline([
    ('tvec', TfidfVectorizer()), 
    ('lr', LogisticRegression(max_iter = 500))
])


pipe_lr_tvec_params = {
    'tvec__max_features':[2000, 3000, 4000, 5000],
    'tvec__stop_words': [None, 'english'], 
    'tvec__ngram_range': [(1, 1), (1, 2)], 
}


gs_lr_tvec = GridSearchCV(pipe_lr_tvec,
                      param_grid=pipe_lr_tvec_params,
                      cv = 5,
                      verbose = 2, 
                      n_jobs = -1 
                      )


gs_lr_tvec.fit(X_train, y_train)


print(f'Best Score: {gs_lr_tvec.best_score_}')
print()
print(f'Best params: {gs_lr_tvec.best_params_}')
print()
y_pred = gs_lr_tvec.predict(X_test)
print(classification_report(y_test, y_pred))


plot_confusion_matrix(y_test, y_pred, 'Logistic Regression + TF-IDFVectorizer')








# Logistic Regression, with TVEC and vocab 
pipe_lr_tvec_vocab = Pipeline([
    ('tvec', TfidfVectorizer(vocabulary = unique_words)), 
    ('lr', LogisticRegression(max_iter = 500))
])


pipe_lr_tvec_vocab_params = {
    'tvec__max_features':[2000, 3000, 4000, 5000],
    'tvec__stop_words': [None, 'english'], 
    'tvec__ngram_range': [(1, 1), (1, 2)], 
}


gs_lr_tvec_vocab = GridSearchCV(pipe_lr_tvec_vocab,
                      param_grid=pipe_lr_tvec_vocab_params,
                      cv = 5,
                      verbose = 2, 
                      n_jobs = -1 
                      )


gs_lr_tvec_vocab.fit(X_train, y_train)


print(f'Best Score: {gs_lr_tvec_vocab.best_score_}')
print()
print(f'Best params: {gs_lr_tvec_vocab.best_params_}')
print()
y_pred = gs_lr_tvec_vocab.predict(X_test)
print(classification_report(y_test, y_pred))


plot_confusion_matrix(y_test, y_pred, 'Logistic Regression + TF-IDFVectorizer (Custom Vocab)')








pipe_nb_cvec = Pipeline([
    ('cvec', CountVectorizer(stop_words='english')),
    ('nb', MultinomialNB())
])


pipe_nb_cvec_params = {
    'cvec__max_features': [1000, 1500, 2000],
    'cvec__ngram_range': [(1, 1)],
    'nb__alpha': [0.9, 1, 2, 3]  
}


gs_nb_cvec = GridSearchCV(pipe_nb_cvec, 
                          pipe_nb_cvec_params, 
                          cv=5, 
                          n_jobs=-1) 


gs_nb_cvec.fit(X_train, y_train)


print(f'Best Score: {gs_nb_cvec.best_score_}')
print()
print(f'Best params: {gs_nb_cvec.best_params_}')
print()
y_pred = gs_nb_cvec.predict(X_test)
print(classification_report(y_test, y_pred))


plot_confusion_matrix(y_test, y_pred, 'Naive Bayes + CountVectorizer')








pipe_nb_tvec = Pipeline([
    ('tvec', TfidfVectorizer(stop_words='english')),
    ('nb', MultinomialNB())
])


pipe_nb_tvec_params = {
    'tvec__max_features':[2000, 3000, 4000, 5000],
    'tvec__stop_words': [None, 'english'], 
    'tvec__ngram_range': [(1, 1), (1, 2)], 
}


gs_nb_tvec = GridSearchCV(pipe_nb_tvec, 
                          pipe_nb_tvec_params, 
                          cv=5, 
                          n_jobs=-1) 


gs_nb_tvec.fit(X_train, y_train)


print(f'Best Score: {gs_nb_tvec.best_score_}')
print()
print(f'Best params: {gs_nb_tvec.best_params_}')
print()
y_pred = gs_nb_tvec.predict(X_test)
print(classification_report(y_test, y_pred))


plot_confusion_matrix(y_test, y_pred, 'Naive Bayes + TF-IDFVectorizer')





data = {
    'Model': ['LogReg + CountVectorizer', 'LogReg + TF-IDF', 'NaiveBayes + CountVectorizer', 'NaiveBayes + TF-IDF'],
    'Precision (Class 0)': [0.82, 0.85, 0.82, 0.87],
    'Recall (Class 0)': [0.78, 0.77, 0.81, 0.72],
    'F1-Score (Class 0)': [0.80, 0.81, 0.81, 0.79],
    'Precision (Class 1)': [0.86, 0.86, 0.87, 0.83],
    'Recall (Class 1)': [0.89, 0.91, 0.88, 0.93],
    'F1-Score (Class 1)': [0.87, 0.88, 0.88, 0.88],
    'Accuracy': [0.84, 0.85, 0.85, 0.84]
}
df = pd.DataFrame(data)
df






